{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilidad I:\n",
    "Valor esperado e indicadores. Teorema de Bayes. Estimación Bayesiana. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. PMFs y PDFs conjuntas.\n",
    "\n",
    "- Una PMF conjunta $p_{X,Y}$ de las variables $X$ y $Y$ está definida como \n",
    "\n",
    "\\begin{equation}\n",
    "p_{X,Y}(x,y)=P(X=x,Y=y)\n",
    "\\end{equation}\n",
    "\n",
    "- La PMF marginal de X y Y puede ser obtenida a partir de la PMF conjunta, utilizando \n",
    "\n",
    "\\begin{equation}\n",
    "p_{X}(x)=\\sum_{y}{p_{X,Y}(x,y)}\n",
    "\\end{equation}\n",
    "y\n",
    "\\begin{equation}\n",
    "p_{Y}(y)=\\sum_{x}{p_{X,Y}(x,y)}\n",
    "\\end{equation}\n",
    "\n",
    "De manera análoga, para $X$ y $Y$ que sean variables aleatorias conjuntas con una PDF conjunta $f_{X,Y}$ se tiene que \n",
    "\n",
    "\\begin{equation}\n",
    "f_{X,Y}(x,y)=f_{Y}(y)f_{X|Y}(x|y)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "f_{X}(x)=\\int_{-\\infty}^{\\infty}f_{Y}(y)f_{X|Y}(x|y)dy\n",
    "\\end{equation}\n",
    "\n",
    "#### Ejercicio 1: Realice dos gráficas de funciones de distribución conjuntas para variables aleatorias X,Y distribuidas normales para medias y varianzas iguales y medias y varianzas diferentes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Valor Esperado y varianza.\n",
    "\n",
    "Para variables discretas se define el valor esperado de una variable discreta $X$ como \n",
    "\n",
    "\\begin{equation}\n",
    "E[X] =\\sum_{x}{x p_{X} (x)}\n",
    "\\end{equation}\n",
    "\n",
    "y para una variable continua:\n",
    "\n",
    "\\begin{equation}\n",
    "E[X] =\\int_{x}{x p_{X} (x)}dx\n",
    "\\end{equation}\n",
    "\n",
    "y la varianza se define como\n",
    "\n",
    "\\begin{equation}\n",
    "var[X] = E[(X-E[X])^2]\n",
    "\\end{equation}\n",
    "\n",
    "#### Ejercicio 2: Obtenga numéricamente el valor esperado y la varianza de una función de distribución Gamma generada aleatoriamente con parámetros $1/\\lambda=0.5$ y $k=9$. Realice la gráfica de la distribución y de la CDF.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Teorema de Bayes\n",
    "\n",
    "Volviendo a los fundamentos de probabilidad, hablemos del teorema de probabilidad total, que dicta: Sean $A_1, ..., A_n$ eventos disjuntos que forman una partición del espacio muestral (cada posible resultado está incluido en exactamente uno de los eventos $A_1, ...., A_n$) y asumamos que $P(A_i)>0$ para todo $i$. Entonces, para cada evento $B$, se tiene \n",
    "\n",
    "\\begin{equation}\n",
    "P(B)=P(A_1 \\cap B)+P(A_n \\cap B)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "=P(A_1)P(B|A_1)+...+P(A_n)P(B|A_n).\n",
    "\\end{equation}\n",
    "\n",
    "este teorema lo que dice es que, intuitivamente, estamos particionando el espacio muestral en un número de escenarios $A_i$. Entonces, la probabilidad que $B$ ocurra es un promedio ponderado de la probabilidad condicional sobre cada escenario, donde cada escenario es pesado de acuerdo con su probabilidad. Esto permite calcular la probabilidad de varios eventos $B$ para los cuales las probabilidades $P(B|A_i)$ son conocidas o fáciles de obtener. El teorema de probabilidad total puede ser aplicado repetidamente para calcular probabilidades en experimentos que tienen caracter secuencial. \n",
    "\n",
    "\n",
    "## Inferencia y regla de Bayes. \n",
    "\n",
    "El teorema de probabilidad total es usualmente usado en conjunto con el teorema de Bayes, que relaciona las probabilidades condicionales de la forma $P(A|B)$ cno probabilidades condicionales de la forma $P(B|A)$, en el cual el orden del condicionamiento es contrario. \n",
    "\n",
    "#### Regla de Bayes\n",
    "Sean $A_1, A_2, ..., A_n$ eventos disjuntos que forman una partición del espacio muestral, y asumiendo que $P(A_i)>0 $ para todo $i$. Entonces para algún evento tal que $P(B)>0$, se tiene\n",
    "\\begin{equation}\n",
    "P(A_i|B)=\\frac{P(A_i)P(B|A_i)}{P(B)}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "=\\frac{P(A_i)P(B|A_i)}{P(A_1)P(B|A_1)+...+P(A_n)P(B|A_n)}\n",
    "\\end{equation}\n",
    "\n",
    "La regla de Bayes es usualmente usada para inferencia. Existen un cierto número de \"causas\" que producen un \"efecto\". Es posible observar el efecto y se desea inferir la causa. Los eventos $A_1,...,A_n$ son asociados con las causas y el evento $B$ representa el efecto. La probabilidad $P(B|A_i)$ de que el efecto observado cuando la causa $A_i$ está presente, contribuye a un modelo probabilistico de relación de causa y efecto. Dado el efecto B que se ha observado, se desea evaluar la probabilidad $P(A_i|B)$ de que la causa $A_i$ esté presente. En general se refiere a $P(A_i|B)$ como la probabilidad a posteriori de un evento $A_i$ y a $P(A_i)$ la cual se denomina probabilidad a priori. \n",
    "\n",
    "##### Nota:\n",
    "\n",
    "Es importante recordar el concepto de independencia, que dice que cuando $P(A|B)=P(A)$, entonces A es independiente de B. Por lo que por definición:\n",
    "\n",
    "\\begin{equation}\n",
    "P(A\\cup B)=P(A)P(B)\n",
    "\\end{equation}\n",
    "\n",
    "En el caso en que $P(B)=0$, entonces $P(A|B)$ no está definida, por lo que la relación se mantiene. La simetría de esta relación también implica que la independencia es una propiedad simétrica; es decir, que si A es independiente de B, entonces B es independiente de A, y se puede decir que A y B son eventos independientes. \n",
    "\n",
    "Además, también es importante notar que la independencia condicional entre un evento C y dos eventos B y C, es condicional si\n",
    "\n",
    "\\begin{equation}\n",
    "P(A\\cup B| C)=P(A|C)P(B|C)\n",
    "\\end{equation}\n",
    "\n",
    "Que significa, en otras palabras, que si se sabe que C ocurrió, el conocimiento adicional de que B también ocurrió no cambia la probabilidad de A. La independencia entre dos eventos A y B respecto a una ley de probabilidad incondicional no implica la independencia condicional y viceversa. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Inferencia Bayesiana\n",
    "\n",
    "La inferencia estadística es el proceso de extraer información a partir de una variable o un modelo desconocido a partir de información disponible. \n",
    "\n",
    "La infrencia estadística difiere de la teoría de probabilidad en formas fundamentales. La probabilidad es un área de las matemáticas completamente autocontenida, basada en axiomas, como ya se ha visto. En razonamiento probabilístico se asume un modelo probabilístico completamente especificado que obedece estos axiomas. Luego se hace uso de un método matemático para cuantificar las consecuencias de este modelo o responder varias preguntas de interés. En particular, cada pregunta no ambigua tiene una respuesta correcta única. \n",
    "\n",
    "La estadística es diferente. Para cualquier problema, pueden existir múltiples métodos razonables, con diferentes respuestas. En general, no hay una forma de obtener el mejor método, a menos que se realicen suposiciones fuertes y se impongan restricciones adicionales sobre la inferencia. \n",
    "\n",
    "\n",
    "En la estadística Bayesiana, todas las suposiciones se localizan en un lugar, en la forma de un prior, los estadístas Bayesianos argumentan que todas las suposiciones son traidas a una superficie y están dispuestas al escrutinio. \n",
    "\n",
    "Finalmente hay consideraciones prácticas. En muchos casos, los métodos Bayesianos son computacionalmente intratables. Sin embargo, con las capacidades de cómputo recientes, gran parte de la comunidad se enfoca en realizar métodos Bayesianos más prácticos y aplicables. \n",
    "\n",
    "\n",
    "Conceptos clave:\n",
    "\n",
    "- En estadística Bayesiana, se tratan los parámetros desconocidos como variables aleatorias con distribuciones a priori conocidas. \n",
    "\n",
    "- En estimación de parámetros se quiere generar un estimado de qué tan cercanos están los valores de los estimadores a los verdaderos valores de los parámetros en un sentido probabilístico. \n",
    "\n",
    "- En pruebas de hipótesis, el parámetro desconocido toma uno de valores infinitos, correspondiente según la hipótesis. Se quiere seleccionar una hipótesis basados en una pequeña probabilidad de error. \n",
    "\n",
    "- Los principales métodos de inferencia Bayesiana son:\n",
    "\n",
    "\t- MAP: (Maximun a posteriori probability): A partir de posibles valores de parámetros, se selecciona uno con máxima probabilidad condicional dados unos datos. \n",
    "\n",
    "\t- LMS: (Least Mean Squares) Se selecciona un estimador/función de los datos que minimiza el error cuadrático medio entre el parámetro y su estimado. \n",
    "\n",
    "\t- Linear Least Mean Squares: Se selecciona un estimador que es una función lineal de los datos y minimiza el error cuadrático medio entre los parámetros y su estimado. \n",
    "    \n",
    "    \n",
    "    \n",
    "En inferencia bayesiana, la cantidad de interés se denota por $\\Theta$, y es modelada como una variable aleatoria o como una colección finita de variables aleatorias. Aquí, $\\theta$ puede representar cantidades físicas, tales como una velocidad o una posición, o un conjunto de parámetros desconocidos de un modelo probabilístico. Por simpleza, a menos que lo contrario sea explícitamente mencionado, se ve $\\Theta$ como una variable aleatoria.\n",
    "\n",
    "El objetivo es extraer información acerca de $\\theta$, basado en observar una colección de variables aleatorias $X=(X_1,...,X_n)$ relacionadas, llamadas observaciones, medidas o vector de observaciones. Por esto, se asume que se conoce la distribución conjunta de $\\Theta$ y $X$. Es decir, \n",
    "\n",
    "- Se asume el conocimiento de la distribución a priori $p_\\theta$, dependiendo de si $\\theta$ es discreta o continua-\n",
    "\n",
    "- Una distribución condicional $p_{X|\\theta}$, dependiendo de si X es discreta o continua. \n",
    "\n",
    "Una vez un valor particular de $x$ en $X$ ha sido observado, una respuesta completa de un problema de inferencia es proveído por la distribución a posteriori. Esta distribución está determinada por la forma apropiada de la regla de Bayes y encapsula todo el conocimiento que se pueda tener acerca de $\\Theta$ dada la información disponible. \n",
    "\n",
    "### Resumen:\n",
    "\n",
    "- Se comienza con una distribución a priori $p_{\\Theta}$ o $f_{\\Theta}$ para una variable desconocida aleatoria $\\Theta$.\n",
    "\n",
    "- Se tiene un modelo $p_{X|\\Theta}$ o $f_{X|\\Theta}$ del vector se observaciones X. \n",
    "\n",
    "- Después de observar el valor $x$ en $X$, se forma la distribución a posteriori de $\\Theta$, usando la versión apropiada de la regla de Bayes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ejercicio:\n",
    "Suponga que tratamos con las dos siguientes hipótesis\n",
    "\n",
    "\\begin{equation}\n",
    "H_1:p=0.1,H_2:p=0.2\n",
    "\\end{equation}\n",
    "\n",
    "donde $p=0.2$ está basada en una proporción muestral de 1:5. \n",
    "\n",
    "Primero asumamos que el tamaño de la muestra es $n=5$, $k=1$. También podemos asumor que la probailidad a priori es 0.5 y 0.5, es decir igual para ambas hipótesis. Es posible actualizar la probabilidad a posteriori de cada hipótesis utilizando la regla de Bayes. Calcule las probabilidades P(p=0.1) y P(p=0.2) respectivamente para muestras de tamaño 5, 10, 15 y 20 y concluya al respecto. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Referencias.\n",
    "\n",
    "- Wackerly, D. Mathematical Statistics with Applications. 2008\n",
    "- Bertsekas, D. Introduction to Probability. 2008.\n",
    "- Sivia, D.S. Data Analysis: A Bayesian Tutorial. 2006."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
