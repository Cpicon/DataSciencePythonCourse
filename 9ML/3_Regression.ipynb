{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Regresión\n",
    "\n",
    "El objetivo principal de la regresión es predecir el valor de uno o más targets continuos $t$ dado el valor de un vector $D-dimensional$ de entrada $x$. La forma más simple de modelos de regresión lineal consisten de funciones lineales en las entradas. Sin embargo, es posible obtener otras clases de funciones lineales a partir de combinaciones lineales de un conjunto de funciones no lineales de entrada llamadas funciones base. Tales modelos, son lineales en los parámetros, lo que permite que tengan las mismas propiedades analíticas con las que se tratan en el caso de funciones de entrada lineal, y aún así, ser no lineales respecto a las variables de entrada. \n",
    "\n",
    "Dado un conjunto de datos de entrada que contenga $N$ observaciones ${x_n}$, donde $n=1,...,n$ junto con sus vectores respuesta correspondientes ${t_n}$, el objetivo es predecir el valor $t$ para un nuevo valor de $x$. En la aproximación más simple, esto se puede realizar construyendo una función apropiada $y(x)$ cuyos valores para nuevas entradas de $x$ constituyen predicciones para valores correspondientes de $t$.\n",
    "\n",
    "## 3.1 Modelos de funciones base lineales. \n",
    "\n",
    "El modelo más simple para regresión involucra una combinación lineal de variables de entrada:\n",
    "\n",
    "\\begin{equation}\n",
    "y(x,w)=w_0 + w_1 x_1+w_2 x_2+...+w_D x_D\n",
    "\\end{equation}\n",
    "\n",
    "En general, esto es lo que se conoce como regresión lineal. La propiedad clave de este modelo es que es una función lineal de los parámetros $w_i$ y también es una función lineal de $x_i$ y esto impone limitaciones importantes al modelo. Sin embargo, es posible extender este modelo considerando combinaciones lineales de funciones no lineales fijas de las variables de entrada de la forma\n",
    "\n",
    "\\begin{equation}\n",
    "y(x,w)=w_0 + \\sum_{j=1}^{M-1}{w_j \\phi_j(x)}\n",
    "\\end{equation}\n",
    "\n",
    "donde $\\phi_j(x)$ son conocidas como funciones base. El número total de parámetros de este modelo, claramente es M. El parámetro $w_0$ permite ajustar un offset en los datos y en general se conoce como parámetro \"bias\" (no confundir con el bias estadístico). En general, es conveniente expresar $\\phi_0(x)=1$, de manera que \n",
    "\n",
    "\\begin{equation}\n",
    "y(x,w)=\\sum_{j=0}^{M-1}{w_j \\phi_j(x)}=w^T \\phi(x)\n",
    "\\end{equation}\n",
    "\n",
    "donde $w=(w_0,...,w_{M-1})^T$ y $\\phi=(\\phi_0,...,\\phi_{M-1})^T$. En muchas aplicaciones prácticas, es posible aplicar alguna forma de preprocesamiento fijo que permita hacer extracción de características (feature extraction), de las variables de datos originales. Si las variables originales se componen del vector x, entonces, las características pueden ser expresadas en términos de los funciones base ${\\phi_j(x)}$.\n",
    "\n",
    "La regresión polinomial es un caso particular de este modelo, en el cual se tiene una única variable de entrada $x$ y las funciones base toman la forma de potencias de $x$ tal que $\\phi_j(x)=x^j$. Una limitación de las funciones base polinomiales es que son funciones globales de la variable de entrada, de manera que los cambios en una región del espacio de entrada afecta las demás regiones. Esto puede ser resuelto al dividir el espacio de entrada en diferentes regiones y ajustar a diferentes polinomios en cada región. \n",
    "\n",
    "Un ejemplo de una posible función base sería \n",
    "\n",
    "\\begin{equation}\n",
    "\\phi_j(x)=exp{-(x-\\mu_j)^2/2s^2}\n",
    "\\end{equation}\n",
    "\n",
    "donde $\\mu_j$ se refiere a la ubicación de las funciones base en el espacio de entrada y $s$ a su escala espacial. Otra posibilidad es una base sigmoidal de la forma \n",
    "\n",
    "\\begin{equation}\n",
    "\\phi_j(x)=\\sigma((x-\\mu_j)/s)\n",
    "\\end{equation}\n",
    "\n",
    "donde $\\sigma(a)$ es la función sigmoide logística. También se puede utilizar la función \"tanh\" que se relaciona con la función logística como $tanh(a)=a\\sigma(a)-1$. También es posible escoger una base en fourier que daría lugar a expansiones en funciones sinoidales. Dados estos argumentos, es claro que lo que se ha visto previamente acerca de Maximum likelihood y Mínimos cuadrados es claramente aplicable para cualquier $\\phi(x)$ que constituya una base de funciones apropiada, sólamente que hasta ahora hemos manejado $\\phi(x)=x$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mínimos cuadrados con regularización.\n",
    "\n",
    "Partamos de una función de error $E_D(w)$ y añadamos un término de regularización para controlar el overfitting, de manera que el error total de la función sea $E_D(w)+\\lambda E_W(w)$\n",
    "\n",
    "donde $\\lambda$ es el coeficiente de regularización y controla la importancia relativa del error dependiente de los datos $E_D(w)$ y el término de regularización es $E_W(w)$. Una forma simple de regularización constituye en sumar la suma de cuadrados de los pesos, ie. ($E_W(w)=1/2 w^Tw$). De manera que, considerando un \n",
    "\n",
    "\\begin{equation}\n",
    "E(w)=\\frac{1}{2}\\sum_{n=1}^{N}{(t_n-w^T \\phi(x_n))^2}\n",
    "\\end{equation}\n",
    "\n",
    "de manera que el error total es \n",
    "\n",
    "\\begin{equation}\n",
    "E(w)=\\frac{1}{2}\\sum_{n=1}^{N}{(t_n-w^T \\phi(x_n))^2}+\\frac{\\lambda}{2}{w^T w}\n",
    "\\end{equation}\n",
    "\n",
    "Esta escogencia de regularización se conoce como \"weight decay\" debido a que en los algoritmos secuenciales, hace que los pesos tiendan a decaer hacia cero. En estadística, proveen un ejemplo de \"encogimiento de parámetros\" (parameter shrinkage) debido a que encoge los parámetros hacia cero. Esto tiene la ventaja de que la función de error permanece siendo una función cuadrática de $w$, por lo que el minimizador puede ser encontrado de forma cerrada. La solución al problema de mímimos cuadrados, se vuelve entonces:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "w=(\\lambda I+\\Phi^T\\Phi)^{-1}\\Phi^T t\n",
    "\\end{equation}\n",
    "\n",
    "Una forma más general de un regularizador tiene la siguiente forma:\n",
    "\n",
    "\\begin{equation}\n",
    "E(w)=\\frac{1}{2}\\sum_{n=1}^{N}{(t_n-w^T \\phi(x_n))^2}+\\frac{\\lambda}{2}{|w_j|^q}\n",
    "\\end{equation}\n",
    "\n",
    "donde $q=2$ corresponde al regularizador cuadrático. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
